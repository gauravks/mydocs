\chapter{Evaluation}
\label{cha:Evaluation}

This chapter presents the details of the evaluation structure and the results
captured. The first section focusses on evaluation of the topologies prototypes
developed to give an understanding of achievable bandwidth with the different
topologies. The second section would capture the comparative evaluation of
the MIDG2 implementations developed in the thesis and the reference base MIDG2
MPI implementation to understand the benefits of the optimization due to introduction
of IO channels.

\section{Topology Evaluation}

The first set of evaluation tests was carried out using the prototypes
developed for the different topologies which are explained in chapter \ref{cha:topologies}.
The aim of the tests was to measure the bandwidth possible to achieve with the
developed OpenCL kernels for the topologies and compare it with the
the communication architecture which utilizes HOST to communicate the
FPGA data.

\subsection{Setup}

The experiments were conducted with the FPGA partition of the Noctua
cluster. 4 Nodes at maximum was utilized. The detailed machine configuration
of the systems is given in table \todo{add table}. Each of the nodes
contain 2 Intel Xeon Gold "Skylake" 6148(F), 2.4 GHz CPUs and
2 Bittware 520N\footnote{\url{https://www.bittware.com/fpga/520n/}}
cards which have Intel Stratix 10 FPGA with 32 GiB memory. The Nodes
are connected with Intel Omni Path 100 Gbps network which is used
for data communication between the CPUs using MPI. The FPGAs
are connected using a point-to-point links using Finisar FTL410QE2C
or Edge Finisar FTL410QE3C compatible QSFP modules.

The bandwidth evaluation is done by communicating data of different
sizes between the nodes in a ping-ping pattern in all the designs.
The ping-ping pattern is the ideal candidate as it maximizes
the utilization of the duplex point-to-point channels in the
FPGAs. To use as a reference to the current MIDG2 implementation,
an additional prototype is developed which performs the communication
between the FPGAs via HOST. In this prototype, the data from the FPGAs
is read over the PCIe and transferred by the HOST over the Intel
Omni path network to the other HOST. On receipt, the second HOST
copies the data into the FPGA over the PCIe to complete a single data
transactions. The data communication between HOSTs is performed using
Intel® MPI Library v18.0.3. Listing \ref{code:mpi_proto} shows the pseudo-code of the section
performing the communication. This prototype simulates the data communication
behavior in the base MPI MIDG2 FPGA implementation and is used to compare the
bandwidth performance and possible improvements using FPGA-to-FPGA communication
on the target hardware systems. The evaluation performed in this
section compares the bandwidth of the FPGA-to-FPGA communication
in  different topologies as well as with the Intel® Omni Path + PCIe based
data communication between the nodes as shown in \ref{}.

\begin{CppCode}[caption=Pseudo-code to perform MPI+PCIe based data communication
    between FPGAs, frame=tlrb, label=code:mpi_proto]
clock_gettime(CLOCK_MONOTONIC, &execStart);
readQ.enqueueReadBuffer(readBuffer, CL_TRUE, 0, buffersize*(nprocs), send_buffer);
for (int p: nodeList)
{
    if (p != procid)
    {
        MPI_Isend(send_buffer[p], buffersize,, p,,);
        MPI_Irecv((rcv_buffer[p], buffersize,, p,,);
    }
}

MPI_Waitall(req, mpi_in_requests,);
MPI_Waitall(req, mpi_out_requests,);

processBuffQ.enqueueWriteBuffer(writeBuffer, CL_TRUE, 0, buffersize*(nprocs), rcv_buffer);
clock_gettime(CLOCK_MONOTONIC, &execEnd);
\end{CppCode}

Two different types of data sizes are used for the evaluation. The first set of data
sizes named as \texttt{regular} are multiples of 32 which produce a aligned data transactions
on the FPGA channels without requirement of any extra padding or alignment.
The second set named as \texttt{irregular} are data sizes taken from the actual
MIDG2 communication pattern for 2 node system. These data sizes produce
non-aligned communication on the channels and are explicitly required to be
aligned/padded to ensure aligned 32 byte writes and read on the channel.
The use of the different data size types was to evaluate the effects of additional
padding requirements which is necessary in most of the applications.

Another set of variations evaluated with the FPGA prototypes is the use of
interleaved or non-interleaved memory partitions for global memory of the FPGA.
As explained in the section \todo{add ref}, the global memory on the Bittware 520N board
contains 4 channels to access global memory. The channels can begin
either configured to be used in a bus-interleaved or non-interleaved fashion.
The non-interleaved global memory access allows the user to specify the
memory channel to be used for accessing the specific buffer. These
memory partition affects the global memory access bandwidth and
the topologies were compared to identify the configuration which performs
best for the topology. The evaluation was done for 5 designs with Each
having 4 variation giving a total of 20 different data sets listed with
description for understanding the following data

\subsection{Bandwidth Comparison}

The bandwidth in the topology designs is computed by measuring the execution time
of the \texttt{send} kernels using \texttt{CL\_PROFILING\_COMMAND\_START}
and \texttt{CL\_PROFILING\_COMMAND\_END} profile counters which give the start and
end time of the kernels in nanoseconds. As ping-ping pattern is used for the
communication in which all the nodes communicating, start sending data simultaneously.
As FPGAs have full duplex channels for communication, there would be no interference
observed for the send and receives. For each data size the the communication is performed
for 100 rounds. The buffer to be exchanged is initialized every round with a value derived from the
element index and the index of the round in which the data is transferred. On receipt
the data is checked to verify the that the data is transferred correctly over the channels
or the Ethernet and PCIe. After each execution, the execution time of the send kernel
is computed using the counters values and the bandwidth for that run is computed using the formula
$$ bandwidth (MB/Secs) = \frac{Data Size (in MB)}{exectime (in seconds)} $$
The bandwidth are accumulated for each round and then the average bandwidth for the
data size is computed after the end of the 100 rounds and stored in file.

The computation in the MPI+PCIe prototype is similar but instead of using the kernel execution
time, the complete time for data transfer from FPGA->CPU over PCIe, MPI transfer and data transfer from
CPU->FPGA over PCIe is used. This time is computed using \texttt{clock\_gettime} as shown in the code
\ref{code:mpi_proto}.

The theoretical peak values for each of the communication patterns is listed in the table \ref{tab:peak_bw}.
The peak bandwidth of the MPI+PCIe design is a combination of the bandwidths of the PCIe
and the Intel® Omni Path. As the data is transferred between the FPGA to HOST and HOST to FPGA
in a store and forward manner, the effective bandwidth of the communication can be computed as:
\begin{equation}\label{eqn:peakbw_mpipcie}
 P_{mpipcie} = \frac{N}{\frac{N}{P_{PCIe}}+\frac{N}{P_{IOP}}+\frac{N}{P_{PCIe}}}
 = \frac{N}{\frac{N}{7.88 GB/s}+\frac{N}{12.5 GB/s}+\frac{N}{7.88 GB/s}} = 2.995 GB/s
\end{equation}
Each external channel of the FPGA is operated at 40 Gbits/s giving a total of 20 GB/s peak
bandwidth for all the channels. Within node topology either uses 1 channel or all 4 and in the
fully connected topology only three channels is used.

\begin{table}[ht]
    \centering
    \caption{Peak bandwidth in each configuration}
    \label{tab:peak_bw}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Configuration} & \textbf{\begin{tabular}[c]{@{}c@{}}Peak Bandwidth\\ (GB/s)\end{tabular}} \\ \hline
    MPI+PCIe               & 2.995                          \\ \hline
    Within Node 1 Channel  & 5                              \\ \hline
    Within Node 4 Channel  & 20                             \\ \hline
    Fully Connect          & 15                             \\ \hline
    \end{tabular}%
\end{table}


Plots in figure \ref{plot:wn} shows the bandwidth variation for Within node design using single channel
and all the 4 channels. Irregular data sizes perform similar to regular data sizes which suggests that
there are no major over head involved with the handling of the irregular data types in applications.
For the single channel design, there are no effects of the global memory interleaving as the slower
external channel is the bottleneck instead of the global memory. On the other hand, for the 4 channels,
use of non-interleaved memory affects the bandwidth performance and the effective bandwidth peak bandwidth
is 16.41 GB/s which is 82 percentage of the peak bandwidth. This is due to the overheads and lower occupancy
of the global memory reads/write from the \texttt{send} and \texttt{write}. The kernels read/write 4*256 bits
per request from the same memory channel causing which is not possible. The global memory channels have a
maximum width of 512 bits and requests of memory width wider sizes would increase te latency and
stalls. While using the interleaved memory, the buffers are stored in all the four memory channels and
the kernels can access them simultaneously. This reduces the latency and stalls for the reads and writes
allowing to reach the active bandwidth of 19.84 GB/s which is 99.2 percentage of the peak bandwidth.

\todo{check the profiling for interleaved and see what happens}

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/wn.pgf}}
    \caption{Bandwidth variation of Within Node designs}
    \label{plot:wn}
\end{figure}


For the fully connected topology also, the bandwidth is not affected by irregular data sizes. The non-interleaved
and interleaved memory designs have similar bandwidth performance for each data type. This is because
the memory channels are explicitly mapped to use separate memory channel for each buffer in the host application.
The explicit mapping assigns individual memory channels to each buffer in global memory. Each of the memory
is used only to read/write data to/from a specific external channel which ensures parallel reads
and writes without latency and stalls. The peak bandwidth with of all the variants of fully connected
design was 14.88 GB/s which is 99.2 percentage of peak 15 GB/s bandwidth possible with 3 channels.
The plot \ref{plot:fc} shows the variation of the bandwidth over the data sizes. The error bars are
added to plot using the measured minimum and maximum bandwidths. The bandwidth has higher variations
for data sizes below 2 MB with a maximum of 2.76 GB/s difference for 256 Kbytes size.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/fc.pgf}}
    \caption{Bandwidth variation of Fully connected design}
    \label{plot:fc}
\end{figure}


The MPI prototypes are only able to achieve a maximum peak bandwidth of 1.94 GB/s with 2 Node system
which is only 65 percentage of the peak bandwidth. The 4 node system only achieves a peak bandwidth of
1.78 GB/s. The main reason for the slower overall bandwidth with 4 nodes is due to the larger
data transfers between the host and the FPGA. The latency of the communication between FPGA-HOST
and HOST-FPGA is much larger than the communication between two HOSTs.There is also a huge difference
in the minimum and maximum bandwidths for all data sizes as shown with the longer error bars in the
plot \ref{plot:mpipcie}.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/mpipcie.pgf}}
    \caption{Bandwidth variation of MPI+PCIe design}
    \label{plot:mpipcie}
\end{figure}

The peak bandwidth observed for the designs in summarized in table and a plot comparing the variation
of the bandwidth over data size is shown in figure \ref{plot:allreg}. The IO channels prototypes are able
to utilize the 99.2\% of the peak bandwidth of using regular and non-regular data sizes in ping-ping
communication pattern. The MPI+PCIe based communication is only able to utilize the 65\% of the available
bandwidth which shows that this communication is less efficient and can be clear bottleneck for applications
which required large data communications.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[ht]
    \centering
    \begin{tabular}{ccccc}
    \multicolumn{1}{c}{\multirow{2}{*}{Designs}} & \multicolumn{2}{c}{Regular} & \multicolumn{2}{c}{Ir-regular} \\ \cline{2-5}
    \multicolumn{1}{c}{} & Interleaved & \multicolumn{1}{c}{Non-interleaved} & Interleaved & \multicolumn{1}{c}{Non-interleaved} \\ \hline
    WN 1CH & \multicolumn{1}{c}{4.96} & 4.96 & \multicolumn{1}{c}{4.96} & 4.96 \\ \hline
    WN 4CH & \multicolumn{1}{c}{19.84} & 16.41 & \multicolumn{1}{c}{19.84} & 16.41 \\ \hline
    FC & \multicolumn{1}{c}{14.88} & 14.88 & \multicolumn{1}{c}{14.88} & 14.88 \\ \hline
    MPI N2 & \multicolumn{2}{c}{1.95} & \multicolumn{2}{c}{1.94} \\ \hline
    MPI N4 & \multicolumn{2}{c}{1.78} & \multicolumn{2}{c}{1.76} \\ \hline
    \end{tabular}%
    \caption{Peak bandwidth observed for each design variant}
    \label{tab:obs_peakbw}
\end{table}


The plot \ref{plot:allreg} also highlights the bandwidth values
of each design at 512 KB data transfer to compare the performance for smaller data sizes which is typical
for some applications. Within node design with 1 channel achieves a bandwidth of 4.22 GB/s which is 84\%
of the peak bandwidth whereas other channel design though utilizing more number of channels is only able
to achieve 62\%(FC) and 64\%(WN 4CH). When compared with fully connected topology, the main reason
for better efficiency is identified as the effect of stalls on the channels. As the kernels in the
within node topology is controlled by single host application, the communication on the channels is
synchronized and produce lesser stalls. For fully connected topology, the FPGAs is controlled
by 2 separate HOST applications which execute independent to each other. This leads to un-synchronized
read and write on the IO channels as the host controls when the kernels execute and access the channels.
This tends to increase the number of stalls as the channels are blocking in nature leading to decrease in
efficiency of the communication. The lower percentage for Within node design is due to the design of the
kernels. In the prototypes tests, same amount of data is transferred over 4 channels as transferred
over one which makes the efficiency lesser as per channel smaller data sizes are communicated. This
is done to keep the communication pattern similar between the nodes in all designs and scaling
happens only by adding more nodes instead of communicating larger data per transfer.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/allreg.pgf}}
    \caption{Comparison of bandwidths of all the designs communicating with regular data sizes.
    The texts show the bandwidths achieved for 512 KBytes data transfers}
    \label{plot:allreg}
\end{figure}


Bandwidth evaluation using prototypes shows the potential benefits of using
the IO channels instead of MPI+PCIe for communication between FPGAs. The IO
channels can be efficiently used utilizing almost complete bandwidth
available for large data transfers. For smaller data sizes within
node communication shows more benefits over other designs. The MPI+PCIe
communication is very less efficient for all data sizes due to high overheads
from the PCIe data transfers.

\section{MIDG2 Execution time evaluation}

As explained in the previous chapters, the IO channels are integrated into the
distributed MIDG2 MPI FPGA implementation and additional optimization are performed
to remove the host interface and reduce the synchronization overheads in the design.
To evaluate the benefits of the changes, the designs are evaluated in this sections
in terms of the speedup of the processing phase of the MIDG2 application. The execution
time of the processing phase which consists the computation of the electric
and magnetic fields using the \ac{DG} method and communication for exchanging the
shared elements data for the distributed system. First, details of the
evaluation setup would be given followed by the evaluation of execution
time and speed up analysis. The second part will present benefits of the IO
channels by evaluating the designs in terms of bandwidth by artificially
increasing the data transfer sizes. In the last, global memory bandwidth
changes in the kernels captured using the profiling interface of the Intel®
OpenCL FPGA SDK would be presented.

\subsection{Evaluation Setup for MIDG2}

\todo{Run test for bw with 4 channels}



\subsection{Speed up Evaluation}

\subsection{Bandwidth}


\subsection{Global Bandwidth}
lower occupancy in the computation kernels giving hints to be computational
bound



