\chapter{Evaluation}
\label{cha:Evaluation}

This chapter presents the details of the evaluation structure and the results
captured. The first section focusses on evaluation of the topologies prototypes
developed to give an understanding of achievable bandwidth with the different
topologies. The second section would capture the comparative evaluation of
the MIDG2 implementations developed in the thesis and the reference base MIDG2
MPI implementation to understand the benefits of the optimization due to introduction
of IO channels.

\section{Topology Evaluation}

The first set of evaluation tests was carried out using the prototypes
developed for the different topologies which are explained in chapter \ref{cha:topologies}.
The aim of the tests was to measure the bandwidth possible to achieve with the
developed OpenCL kernels for the topologies and compare it with the
the communication architecture which utilizes HOST to communicate the
FPGA data.

\subsection{Setup}

The experiments were conducted with the FPGA partition of the Noctua
cluster. 4 Nodes at maximum was utilized. Each of the nodes
contain 2 Intel Xeon Gold "Skylake" 6148(F), 2.4 GHz CPUs and
2 Bittware 520N\footnote{\url{https://www.bittware.com/fpga/520n/}}
cards which have Intel Stratix 10 FPGA with 32 GiB memory. The Nodes
are connected with Intel Omni Path 100 Gbps network which is used
for data communication between the CPUs using MPI. The FPGAs
are connected using a point-to-point links using Finisar FTL410QE2C
or Edge Finisar FTL410QE3C compatible QSFP modules.

The bandwidth evaluation is done by communicating data of different
sizes between the nodes in a ping-ping pattern in all the designs.
The ping-ping pattern is the ideal candidate as it maximizes
the utilization of the duplex point-to-point channels in the
FPGAs. To use as a reference to the current MIDG2 implementation,
an additional prototype is developed which performs the communication
between the FPGAs via HOST. In this prototype, the data from the FPGAs
is read over the PCIe and transferred by the HOST over the Intel
Omni path network to the other HOST. On receipt, the second HOST
copies the data into the FPGA over the PCIe to complete a single data
transactions. The data communication between HOSTs is performed using
Intel® MPI Library v18.0.3. Listing \ref{code:mpi_proto} shows the pseudo-code of the section
performing the communication. This prototype simulates the data communication
behavior in the base MPI MIDG2 FPGA implementation and is used to compare the
bandwidth performance and possible improvements using FPGA-to-FPGA communication
on the target hardware systems. The evaluation performed in this
section compares the bandwidth of the FPGA-to-FPGA communication
in  different topologies with the Intel® Omni Path + PCIe based
data communication between the nodes.

\begin{CppCode}[caption=Pseudo-code to perform MPI+PCIe based data communication
    between FPGAs, frame=tlrb, label=code:mpi_proto]
clock_gettime(CLOCK_MONOTONIC, &execStart);
readQ.enqueueReadBuffer(readBuffer, CL_TRUE, 0, buffersize*(nprocs), send_buffer);
for (int p: nodeList)
{
    if (p != procid)
    {
        MPI_Isend(send_buffer[p], buffersize,, p,,);
        MPI_Irecv((rcv_buffer[p], buffersize,, p,,);
    }
}

MPI_Waitall(req, mpi_in_requests,);
MPI_Waitall(req, mpi_out_requests,);

processBuffQ.enqueueWriteBuffer(writeBuffer, CL_TRUE, 0, buffersize*(nprocs), rcv_buffer);
clock_gettime(CLOCK_MONOTONIC, &execEnd);
\end{CppCode}

Two different types of data sizes are used for the evaluation. The first set of data
sizes named as \texttt{regular} are multiples of 32 which produce a aligned data transactions
on the FPGA channels without requirement of any extra padding or alignment.
The second set named as \texttt{irregular} are data sizes taken from the actual
MIDG2 communication pattern for 2 node system. These data sizes produce
non-aligned communication on the channels and are explicitly required to be
aligned/padded to ensure aligned 32 byte writes and read on the channel.
The use of the different data size types was to evaluate the effects of additional
padding requirements which is necessary in most of the applications.

Another set of variations evaluated with the FPGA prototypes is the use of
interleaved or non-interleaved memory partitions for global memory of the FPGA.
The global memory on the Bittware 520N board
contains 4 channels to access global memory. The channels can begin
either configured to be used in a bus-interleaved or non-interleaved fashion.
The non-interleaved global memory access allows the user to specify the
memory channel to be used for accessing the specific buffer. These
memory partition affects the global memory access bandwidth and
the topologies were compared to identify the configuration which performs
best for the topology. The evaluation was done for 5 designs with Each
having 4 variation giving a total of 20 different data sets listed with
description for understanding the following data

\subsection{Bandwidth Comparison}

The bandwidth in the topology designs is computed by measuring the execution time
of the \texttt{send} kernels using \texttt{CL\_PROFILING\_COMMAND\_START}
and \texttt{CL\_PROFILING\_COMMAND\_END} profile counters which give the start and
end time of the kernels in nanoseconds. As ping-ping pattern is used for the
communication in which all the nodes communicating, start sending data simultaneously.
As FPGAs have full duplex channels for communication, there would be no interference
observed for the send and receives. For each data size the the communication is performed
for 100 rounds. The buffer to be exchanged is initialized every round with a value derived from the
element index and the index of the round in which the data is transferred. On receipt
the data is checked to verify the that the data is transferred correctly over the channels
or the Ethernet and PCIe. After each execution, the execution time of the send kernel
is computed using the counters values and the bandwidth for that run is computed using the formula
$$ bandwidth (MB/Secs) = \frac{Data Size (in MB)}{exectime (in seconds)} $$
The bandwidth are accumulated for each round and then the average bandwidth for the
data size is computed after the end of the 100 rounds and stored in file.

The computation in the MPI+PCIe prototype is similar but instead of using the kernel execution
time, the complete time for data transfer from FPGA->CPU over PCIe, MPI transfer and data transfer from
CPU->FPGA over PCIe is used. This time is computed using \texttt{clock\_gettime} as shown in the code
\ref{code:mpi_proto}.

The theoretical peak values for each of the communication patterns is listed in the table \ref{tab:peak_bw}.
The peak bandwidth of the MPI+PCIe design is a combination of the bandwidths of the PCIe
and the Intel® Omni Path. As the data is transferred between the FPGA to HOST and HOST to FPGA
in a store and forward manner, the effective bandwidth of the communication can be computed as:
\begin{equation}\label{eqn:peakbw_mpipcie}
 P_{mpipcie} = \frac{N}{\frac{N}{P_{PCIe}}+\frac{N}{P_{IOP}}+\frac{N}{P_{PCIe}}}
 = \frac{N}{\frac{N}{7.88 GB/s}+\frac{N}{12.5 GB/s}+\frac{N}{7.88 GB/s}} = 2.995 GB/s
\end{equation}
Each external channel of the FPGA is operated at 40 Gbits/s giving a total of 20 GB/s peak
bandwidth for all the channels. Within node topology either uses 1 channel or all 4 and in the
fully connected topology only three channels is used.

\begin{table}[ht]
    \centering
    \caption{Peak bandwidth in each configuration}
    \label{tab:peak_bw}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Configuration} & \textbf{\begin{tabular}[c]{@{}c@{}}Peak Bandwidth\\ (GB/s)\end{tabular}} \\ \hline
    MPI+PCIe               & 2.995                          \\ \hline
    Within Node 1 Channel  & 5                              \\ \hline
    Within Node 4 Channel  & 20                             \\ \hline
    Fully Connect          & 15                             \\ \hline
    \end{tabular}%
\end{table}


Plots in figure \ref{plot:wn} shows the bandwidth variation for Within node design using single channel
and all the 4 channels. Irregular data sizes perform similar to regular data sizes which suggests that
there are no major over head involved with the handling of the irregular data types in applications.
For the single channel design, there are no effects of the global memory interleaving as the slower
external channel is the bottleneck instead of the global memory. On the other hand, for the 4 channels,
use of non-interleaved memory affects the bandwidth performance and the effective bandwidth peak bandwidth
is 16.41 GB/s which is 82 percentage of the peak bandwidth. This is due to the overheads and lower occupancy
of the global memory reads/write from the \texttt{send} and \texttt{write}. The kernels read/write 4*256 bits
per request from the same memory channel causing which is not possible. The global memory channels have a
maximum width of 512 bits and requests of memory width wider sizes would increase te latency and
stalls. While using the interleaved memory, the buffers are stored in all the four memory channels and
the kernels can access them simultaneously. This reduces the latency and stalls for the reads and writes
allowing to reach the active bandwidth of 19.84 GB/s which is 99.2 percentage of the peak bandwidth.

% \todo{check the profiling for interleaved and see what happens}

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/wn.pgf}}
    \caption{Bandwidth variation of Within Node designs}
    \label{plot:wn}
\end{figure}


For the fully connected topology also, the bandwidth is not affected by irregular data sizes. The non-interleaved
and interleaved memory designs have similar bandwidth performance for each data type. This is because
the memory channels are explicitly mapped to use separate memory channel for each buffer in the host application.
The explicit mapping assigns individual memory channels to each buffer in global memory. Each of the memory
is used only to read/write data to/from a specific external channel which ensures parallel reads
and writes without latency and stalls. The peak bandwidth with of all the variants of fully connected
design was 14.88 GB/s which is 99.2 percentage of peak 15 GB/s bandwidth possible with 3 channels.
The plot \ref{plot:fc} shows the variation of the bandwidth over the data sizes. The error bars are
added to plot using the measured minimum and maximum bandwidths. The bandwidth has higher variations
for data sizes below 2 MB with a maximum of 2.76 GB/s difference for 256 Kbytes size.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/fc.pgf}}
    \caption{Bandwidth variation of Fully connected design}
    \label{plot:fc}
\end{figure}


The MPI prototypes are only able to achieve a maximum peak bandwidth of 1.94 GB/s with 2 Node system
which is only 65 percentage of the peak bandwidth. The 4 node system only achieves a peak bandwidth of
1.78 GB/s. The main reason for the slower overall bandwidth with 4 nodes is due to the larger
data transfers between the host and the FPGA. The latency of the communication between FPGA-HOST
and HOST-FPGA is much larger than the communication between two HOSTs.There is also a huge difference
in the minimum and maximum bandwidths for all data sizes as shown with the longer error bars in the
plot \ref{plot:mpipcie}.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/mpipcie.pgf}}
    \caption{Bandwidth variation of MPI+PCIe design}
    \label{plot:mpipcie}
\end{figure}

The peak bandwidth observed for the designs in summarized in table and a plot comparing the variation
of the bandwidth over data size is shown in figure \ref{plot:allreg}. The IO channels prototypes are able
to utilize the 99.2\% of the peak bandwidth of using regular and non-regular data sizes in ping-ping
communication pattern. The MPI+PCIe based communication is only able to utilize the 65\% of the available
bandwidth which shows that this communication is less efficient and can be clear bottleneck for applications
which required large data communications.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[ht]
    \centering
    \begin{tabular}{ccccc}
    \multicolumn{1}{c}{\multirow{2}{*}{Designs}} & \multicolumn{2}{c}{Regular} & \multicolumn{2}{c}{Ir-regular} \\ \cline{2-5}
    \multicolumn{1}{c}{} & Interleaved & \multicolumn{1}{c}{Non-interleaved} & Interleaved & \multicolumn{1}{c}{Non-interleaved} \\ \hline
    WN 1CH & \multicolumn{1}{c}{4.96} & 4.96 & \multicolumn{1}{c}{4.96} & 4.96 \\ \hline
    WN 4CH & \multicolumn{1}{c}{19.84} & 16.41 & \multicolumn{1}{c}{19.84} & 16.41 \\ \hline
    FC & \multicolumn{1}{c}{14.88} & 14.88 & \multicolumn{1}{c}{14.88} & 14.88 \\ \hline
    MPI N2 & \multicolumn{2}{c}{1.95} & \multicolumn{2}{c}{1.94} \\ \hline
    MPI N4 & \multicolumn{2}{c}{1.78} & \multicolumn{2}{c}{1.76} \\ \hline
    \end{tabular}%
    \caption{Peak bandwidth observed for each design variant}
    \label{tab:obs_peakbw}
\end{table}


The plot \ref{plot:allreg} also highlights the bandwidth values
of each design at 512 KB data transfer to compare the performance for smaller data sizes which is typical
for some applications. Within node design with 1 channel achieves a bandwidth of 4.22 GB/s which is 84\%
of the peak bandwidth whereas other channel design though utilizing more number of channels is only able
to achieve 62\%(FC) and 64\%(WN 4CH). When compared with fully connected topology, the main reason
for better efficiency is identified as the effect of stalls on the channels. As the kernels in the
within node topology is controlled by single host application, the communication on the channels is
synchronized and produce lesser stalls. For fully connected topology, the FPGAs is controlled
by 2 separate HOST applications which execute independent to each other. This leads to un-synchronized
read and write on the IO channels as the host controls when the kernels execute and access the channels.
This tends to increase the number of stalls as the channels are blocking in nature leading to decrease in
efficiency of the communication. The lower percentage for Within node design is due to the design of the
kernels. In the prototypes tests, same amount of data is transferred over 4 channels as transferred
over one which makes the efficiency lesser as per channel smaller data sizes are communicated. This
is done to keep the communication pattern similar between the nodes in all designs and scaling
happens only by adding more nodes instead of communicating larger data per transfer.

\begin{figure}[ht]
    \centering
    \scalebox{0.8}{\input{data/allreg.pgf}}
    \caption{Comparison of bandwidths of all the designs communicating with regular data sizes.
    The texts show the bandwidths achieved for 512 KBytes data transfers}
    \label{plot:allreg}
\end{figure}


Bandwidth evaluation using prototypes shows the potential benefits of using
the IO channels instead of MPI+PCIe for communication between FPGAs. The IO
channels can be efficiently used utilizing almost complete bandwidth
available for large data transfers. For smaller data sizes within
node communication shows more benefits over other designs. The MPI+PCIe
communication is very less efficient for all data sizes due to high overheads
from the PCIe data transfers.

\section{MIDG2 Execution time evaluation}

As explained in the previous chapters, the IO channels are integrated into the
distributed MIDG2 MPI FPGA implementation and additional optimization are performed
to remove the host interface and reduce the synchronization overheads in the design.
To evaluate the benefits of the changes, the designs are evaluated in this sections
in terms of the speedup of the processing phase of the MIDG2 application. The execution
time of the processing phase which consists the computation of the electric
and magnetic fields using the \ac{DG} method and communication for exchanging the
shared elements data for the distributed system. First, details of the
evaluation setup would be given followed by the evaluation of execution
time and speed up analysis. The second part will present benefits of the IO
channels by evaluating the designs in terms of bandwidth by artificially
increasing the data transfer sizes. In the last, global memory bandwidth
changes in the kernels captured using the profiling interface of the Intel®
OpenCL FPGA SDK would be presented.

\subsection{Evaluation Setup for MIDG2}

The experiments like with topologies are conducted on the same
nodes in the FPGA partition of the Noctua cluster. The kernels
for the experiments were synthesized using Intel® FPGA SDK for OpenCL
v18.0.1 Pro (build 261) and v18.1.1 Pro (build 263). Bittware's
p520\_max\_sg280l BSP with both versions is used which provides support
for 1SG280LN2F43E2VG Stratix 10 FPGA with PCIe Gen3x8, 4 memory channels
each of 8GB (32GB total), temperature and power monitoring,
flash programming, 4x40G external I/O channels.

The runtime evaluation of the MIDG2 application is done by using
8 different mesh sizes which starts from 1 thousand elements till
200 thousand elements. This provides the necessary problem size
variation for the evaluation and comparing the speedup capabilities
over the different element sizes. The execution time evaluation
is done for polynomial order of \texttt{p = 3} which is the default
order of the MIDG2 application. Other orders were not evaluated as
additional efforts were required to configure the unrolling factors
in the kernels which was not feasible in this thesis. The table
\ref{tab:constants} lists the values of the constants dependant
on the polynomial order which affect the unrolling factor in the
kernels, memory sizes of the buffers within the kernels and
host as well as the arithmetic intensity of the kernels.
This thesis utilizes the same unrolling factors proposed in
\cite{kenter_opencl-based_2018} for the single FPGA implementation
and the arithmetic intensity of the kernels remain the same.

\begin{table}[ht]
    \centering
    \caption{Constants value as \texttt\{p = 3\}}
    \label{tab:constants}
    \begin{tabular}{lcc}
    \multicolumn{1}{c}{\textbf{Constant}} & \multicolumn{1}{c}{\textbf{Computation}} & \multicolumn{1}{c}{\textbf{Value at p = 3}} \\
    \hline
    VOL\_UNROLL & p\_Np & 20 \\
    SURF\_UNROLL & p\_Nfp & 10\\
    p\_Nfp & ((p + 1)*(p+2)/2) & 10 \\
    p\_Np & ((p + 1)*(p+2)*(p+3)/6) & 20 \\
    p\_max\_NfpNfaces\_Np & max(p\_Nfp*p\_Nfaces,p\_Np) & 40 \\
    BSIZE & (PFAC*((p\_Np+PFAC-1)/PFAC)) & 20 \\
    \hline
    \end{tabular}%
\end{table}

The runtime computation in the MIDG2 application is done using \texttt{clock\_gettime()}
Linux time API. The same time computation is added for all the designs to
measure the processing time in each of the designs. The processing time
for each execution of the host application includes
\begin{itemize}
\item Computation time for computing field values for \texttt{K} mesh elements
executing for \texttt{timestep = 35} and \texttt{finalTime = 0.005}. This along
with 5 Runge-Kutta stages per timestep gives 135 iterations. The \texttt{finalTime}
and the \texttt{timestep} values are fixed in the application instead of
computing using the mesh size to fix the number of computation iterations for each
mesh size. This allows to identify the effects of any latencies which other
due to HOST - FPGA communication.
\item Communication time for transferring \texttt{parNtotal * 4} bytes per iteration
using either IO channels or MPI+PCIe.
\end{itemize}

The host application restructuring made the benchmarking simpler as it is
simpler to separate common functionalities such as runtime computation
in the top level files. The code \ref{code:runtime_comp} show the code
for computing the run time placed in the top level \texttt{BuildCLDevice}
module.
\begin{CppCode}[caption=Runtime computation code for the MIDG2 Designs
    using \texttt{clock\_gettime()}, frame=tlrb, label=code:runtime_comp]
if (opts.profile)
{
    clock_gettime(CLOCK_MONOTONIC, &execStart);
}
// Execute the Kernels
clIntfHandlers->fnRunKernel(mesh, runparams, profileInfo, kernelinfo, &clObjs);

if (opts.profile)
{
    clock_gettime(CLOCK_MONOTONIC, &execEnd);
    exectime = ((execEnd.tv_sec - execStart.tv_sec)*1E9
            + (execEnd.tv_nsec - execStart.tv_nsec));

    profileInfo->totalexectime += exectime;
}
\end{CppCode}

The restructuring also enables to write shell scripts which are
used to invoke the application with different parameters to test different
designs and variants with the same binary. For the evaluation the host
application is modified to include repeated execution of the kernels for the
same mesh for n number of times. This allows to repeat the tests with
same mesh for multiple times and use the arithmetic mean of the execution times
of all the runs for the comparison purpose. All the tests performed repeat
the tests for 30 times for each mesh size. The application code handling
the repetition is shown in listing \ref{code:host_repeat}.
\begin{CppCode}[caption=Test repetion implementation in the host code, frame=tlrb, label=code:host_repeat]
for (int runs = 0; runs < opts.run; runs++)
{
    // Configure the FPGA
    ret = BuildKernels(mesh, initParams, &kernelInfo);
    if (ret != 0)
    {
      printf("Unable to build Kernels exiting\n");
      MPI_Finalize();
      fclose(fp1);
      exit(ret);
    }

    // Run the kernels to compute field values and save execution times
    run_kernels(runParams, initParams, mesh, kernelInfo, nb_kernel);
    cleanup();
    MPI_Barrier(MPI_COMM_WORLD);
}
\end{CppCode}
Every run, the application first configures the FPGA device using \texttt{BuildKernels}.
The configuration involves identifying the platform and the device to be used, creating
context for the device, programming the FPGA device with the provided kernels binary,
creating OpenCL buffers in device memory and then writing the buffers with initial data.
This is followed by setting the parameters in the kernels. Once the configuration is successful,
the application run the kernels by calling \texttt{run\_kernels} routine which invokes the
\texttt{RunKernel\_xxx} routine the enqueue the kernels and perform the computation. Depending
upon the implementation, the timestep iterations are controlled in the host or the FPGA.
The runtime for each runs are computed and after completing the computation the recorded
run times are written into a output csv file. The output CSV contains mesh size,
order of the execution, min, max and avg execution time for the processing, size
of data communicated by the node, the nodal and the L2 Norm errors computed from the
final field values.

The evaluation for compute time is done for 5 different implementations which
include the base design \ref{sec:midge_mpi}, IO channels used in within node topology
fully connected topology with host synchronization \ref{sec:struc_iochan},
IO channels used without host synchronization in within node and fully
connected topologies \ref{sec:final_struc}. Each of the design was synthesized
with both the toolchains which is done to evaluate the difference in the performance
achieved with the newer toolchains. Additional variation due to different memory
configuration parameters available in the Intel® FPGA SDK OpenCL v18.1.1 was
also included. As the latencies included in the FPGA only designs contributes
a large time to the execution of the meshes with lower sizes, another set of
tests were performed for these designs keeping the latency to only 1 cycle.
Table \ref{tab:midg2_designs} lists all the evaluated designs and the variations
along with the names used in the following text, tables and plots for the variants.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
    \centering
    \caption{MIDG2 designs with the variations used to perform the run time evaluation}
    \label{tab:midg2_designs}
    \begin{tabular}{p{3.5cm}C{1.5cm}C{1.5cm}C{2cm}l}
    \textbf{Design} & \textbf{FPGAs used} & \textbf{Mem flags} & \textbf{Added latency} & \textbf{Label} \\
    \hline
    \multirow{4}{*}{\parbox{3.5cm}{MIDG2 MPI FPGA}} & 2 & Yes & No & \textit{MPI\_N2} \\
     & 4 & Yes & No & \textit{MPI\_N4} \\
     \cline{2-5}
     & 2 & No & No & \textit{MPI\_N2\_nf} \\
     & 4 & No & No & \textit{MPI\_N4\_nf} \\
     \hline
    \multirow{2}{*}{\parbox{3.5cm}{Within Node with 1 IO Channel}} & 2 & Yes & No & \textit{WNIO} \\
     & 2 & No & No & \textit{WNIO\_nf} \\
     \hline
    \multirow{2}{*}{\parbox{3.5cm}{Within Node with 4 IO Channels}} & 2 & Yes & No & \textit{WNIO\_4CH} \\
     & 2 & No & No & \textit{WNIO\_4CH\_nf} \\
     \hline
    \multirow{2}{*}{\parbox{3.5cm}{Fully connected with IO Channel}} & 4 & Yes & No & \textit{FCIO} \\
     & 4 & No & No & \textit{FCIO\_nf} \\
     \hline
    \multirow{4}{*}{\parbox{3.5cm}{Within Node FPGA Only 1 Channel}} & 2 & Yes & Yes & \textit{WN} \\
    \cline{2-5}
     & 2 & Yes & No & \textit{WN\_nolat} \\
     \cline{2-5}
     & 2 & No & Yes & \textit{WN\_nf} \\
     \cline{2-5}
     & 2 & No & No & \textit{WN\_nolat\_nf} \\
     \hline
    \multirow{4}{*}{\parbox{3.5cm}{Within Node FPGA Only 4 Channels}} & 2 & Yes & Yes & \textit{WN\_4CH} \\
    \cline{2-5}
     & 2 & Yes & No & \textit{WN\_4CH\_nolat} \\
     \cline{2-5}
     & 2 & No & Yes & \textit{WN\_4CH\_nf} \\
     \cline{2-5}
     & 2 & No & No & \textit{WN\_4CH\_nolat\_nf} \\
     \hline
    \multirow{4}{*}{\parbox{3.5cm}{Fully Conneted FPGA Only}} & 4 & Yes & Yes & \textit{FC} \\
    \cline{2-5}
     & 4 & Yes & No & \textit{FC\_nolat} \\
     \cline{2-5}
     & 4 & No & Yes & \textit{FC\_nf} \\
     \cline{2-5}
     & 4 & No & No & \textit{FC\_nolat\_nf} \\
     \hline
    \end{tabular}%
\end{table}

To perform the test with all of the designs listed in the table \ref{tab:midg2_designs},
the kernels for each design were synthesized with both versions of the tool. To disable
the additional memory optimization which are enabled using the flags \texttt{-global-ring -duplicate-ring}
were disabled by removing the flags in the second synthesis. The kernel binaries for each
of the synthesis are placed in separate directories to be available for use at the same time.
The \ref{tab:syn_freq} lists the frequencies of the synthesized design.

\begin{table}[ht]
    \centering
    \caption{Frequencies for the synthesized design }
    \label{tab:syn_freq}
    \begin{tabular}{lccc}
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Design}}} & \multicolumn{3}{c}{\textbf{Fmax(MHz)}} \\
    \cline{2-4}
    \multicolumn{1}{c}{} & \textbf{18.0.1} & \textbf{18.1.1} & \textbf{18.1.1\_nf} \\
    \hline
    MPI & 312.3 & 283 & 288.68 \\
    WNIO & 281 & 287.35 & 278.86 \\
    FCIO & 262.5 & 223.36 & 244.91 \\
    WNIO\_4CH &  &  &  \\
    WN\_4CH & 254.16 & 248.75 & 231.42 \\
    FPGA ONLY WN & 265.32 & 276.93 & 254.84 \\
    FPGA ONLY FC & 236.4 & 239.46 & 237.41
    \end{tabular}%
\end{table}


\subsection{Speed up Evaluation}

This section presents evaluation results for the runtime evaulation
in terms of speedup of the processing time for different mesh sizes.

\subsubsection*{MPI MIDG2 FPGA vs IO Channels}
The first modification done as part of the thesis is the addition of
IO channels. The aim with addition of IO channels is to reduce the communication
overhead due to PCIe transfers involved in exchanging the non-shared elements
between the nodes. As the communication is overlapped with the execution
as shown in the sequence graph in the figure \ref{fig:sequence_comp},
it is not expected to add any major processing time improvements.
Plot \ref{plot:mpiio_comp} shows the speed up values achieved for the
IO channels based design over the MPI designs.
\begin{figure}[h]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/comp_mpiio_1.pgf}} & \scalebox{0.5}{\input{data/comp_mpiio_2.pgf}} \\
    (a) & (b)
	\end{tabular}
    \caption{MIDG2 MPI FPGA vs MIDG2 FPGA IO Channels designs}
	\label{plot:mpiio_comp}
\end{figure}
The Within Node
topology has higher speed up values then the fully connected designs
for bigger mesh sizes. The speed up for the smaller mesh sizes
are reported to be higher in all the cases as the computation
time for the smaller meshes is smaller and the overall
runtime is affected more by the higher delays in communication.
On the other hand, the large computation time for large meshes
compensates for delays in communication The speed values for the
Within node designs vary between 1.30 to 1.40 which suggests
could be explained as the data sizes in a two node design
would be larger and have more effect on the overall execution
time then in the 4 node designs. Also as it was noticed
in the topology evaluation, the 1 channel topology is able
to achieve higher bandwidth for smaller data sizes which
could be another factor. Comparing the tool chain variation,
it can be noticed that the within node design perform similar
in both the versions whereas for fully connected designs
the 18.1.1 toolchain has lower speedup values which
goes to a minimum of 1.14 for the 200k compared to 1.27
which is around 10\% faster.


\subsubsection*{MPI MIDG2 FPGA vs FPGA ONLY}

As explained in \ref{cha:sys_fpgaonly}, to avoid the host synchronization
which added some amount of overhead to the processing, the second modification
removed the host synchronization completely. These designs were implemented
to utilize the full potential of the FPGA without the host interference.
Plot shows the variation of speedup over different mesh sizes for the
MIDG2 MPI and FPGA only designs. Overall the speed up values for
both design are very low which range for a maximum of 1.34 for the 20k
within node implementation to 1.12 for the fully connected design.
As with the IO channels design, the within node performs better
with higher speed up values for all meshes. Comparing the effects
of toolchain variation, for both the designs the 18.1.1 version
performance slightly better for higher data sizes unlike the IO channels.
\begin{figure}[h]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/comp_mpifpgaonly_1.pgf}} & \scalebox{0.5}{\input{data/comp_mpifpgaonly_2.pgf}} \\
    (a) & (b)
	\end{tabular}
    \caption{MIDG2 MPI FPGA vs MIDG2 FPGA only design}
	\label{plot:mpifpgaonly_comp}
\end{figure}

Comparing the performance of the FPGA ONLY design with IO channels
design it can be seen that the optimization done are not advantageous and
decrease the speedup values. Plot \ref{plot:iofpgaonly_comp} shows the
comparison of the speed values achieved for the IO channels designs and
the FPGA only designs. Both the design see significant decrease in the
speed up values. The impact of the changes is much higher in the case of
the 18.0.1 toolchain where for Fully connect design with 1k Mesh size
there is a decrease of 21\% and for 200k 10\%. For 18.1.1 version
the deviations are only higher for smaller mesh sizes and converge
towards the higher mesh sizes which could be because of the
use of fixed latency time within the kernels which is around
150000 cycles (approx 75ms at 270MHz) which could affect the runtime
of smaller mesh sizes.
\begin{figure}[h]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/iompifpgaonly_all_18_0.pgf}} & \scalebox{0.5}{\input{data/iompifpgaonly_all_18_1.pgf}}\\
    (a) & (b)
	\end{tabular}
    \caption{MIDG2 FPGA IO channels vs MIDG2 FPGA only design}
	\label{plot:iofpgaonly_comp}
\end{figure}


To understand the impact of the latencies in the FPGA only design, the host
application was configured to allow setting up the latency count at the runtime.
This is used to set the latency to 1 and run the tests again to see the impact
of the latency. The field values produced by this change are not accurate, as
the memory operations are not completed correctly and synchronized among the FPGA
though it gives the potential of the FPGA design. The plot \ref{plot:fpgaonly_nolat}
shows the speed up values for each mesh size for the no latency run.
\begin{figure}[h]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/comp_mpifpgaonly_nolat_1.pgf}} & \scalebox{0.5}{\input{data/comp_mpifpgaonly_nolat_2.pgf}}\\
    (a) & (b)
	\end{tabular}
    \caption{MIDG2 FPGA only design with latency set to one cycle count}
	\label{plot:fpgaonly_nolat}
\end{figure}
The plot shows that for smaller mesh sizes very high speed up values which is upto 8.68 for the fully connected
topologies is possible. The speedup values for the fully connected topologies is higher then within node
design which could be explained due to smaller communication times due to simultaneous data transfers
which results in lesser synchronization time delays between the kernels which still use channels
to synchronize but with no latency. For higher data sizes the benefits dimishes completely giving
similar speedup values like the run with latency. This is due to the higher computation times for
bigger meshes. As the mesh size increases, the kernel spend more time for computation. The main
benefit of the FPGA only design comes from removal of the overhead due to host interactions per iterations.
With higher mesh sizes the overhead only contributes a very small fraction of the
execution time per iteration and is not significant for the overall execution making the benefits
of removal of synchronization minimal. In terms of toolchain variation, there are no noticeable
difference in the speedup in either of the design.


\subsubsection*{Effects of Memory flags}

The 18.1.1 toolchain version provides additional memory optimization
flags which control the global memory interconnections to the FPGA
fabric. As explained previously, the designs were also synthesized
with the flags disabled to see the effects on the performance.
The plots in figure \ref{plot:noflag} show the comparison of the speedup
of the IO channel  and FPGA only designs with MPI MIDG2 with and without
memory flags. For FPGA only design the Fully connected topology have similar
speed up and show no effects of the flag but in case of the within node topology,
disabling the flag reduces the speed up marginally for all the mesh sizes but
difference is very small and could be attributed to variations. For the
IO channels the, the within node design has no effect whereas the fully
connected design has benefits for all mesh sizes for about 10\% increase in
speed up.
\begin{figure}[h]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/flag_noflg_iochan.pgf}} & \scalebox{0.5}{\input{data/flag_noflg_fpgaonly.pgf}}\\
    (a) MPI vs IO Channel & (b) MPI vs FPGA Only
	\end{tabular}
    \caption{Effects of memory optimization flags in 18.1.1 tool chain}
	\label{plot:noflag}
\end{figure}

\subsection{Bandwidth evaluation with MIDG2 variants}

The execution time evaluation shows that there are possible benefits of using the IO channels
in terms of improved communication for atleast smaller mesh sizes. With bigger mesh sizes
the benefits of the improvements are not realized as the longer computation part runs
simultaneously. To better understand the benefits of the IO channels, an additional set of
tests were designed. The aim of these tests is to simulate higher sizes of data transfer
in application and compare the bandwidth performance and the runtime in this case.
To achieve higher data size data transfers, the partition configuration was updated to
partition meshes at tetrahedral boundaries instead of triangular surface boundaries.
This is done by updating the \texttt{ncommonnodes} value to 4 from 3, which is the parameter
to control the number of vertices of the element to be used for partitioning. The
size of the data shared in both the designs for a 2 node and 4 node system is
capture in \ref{tab:sizes_var}
\begin{table}[ht]
    \centering
    \caption{Data sizes of the data transfers between then nodes for the two different values of ncommonnodes and number of nodes used}
    \label{tab:sizes_var}
    \begin{tabular}{|ccccc|}
    \hline
    \multirow{2}{*}{Elements} & \multicolumn{2}{c|}{data size 2 node(in KB)} & \multicolumn{2}{c|}{data size 4 Node(in KB)} \\ \cline{2-5}
     & ncom = 3 & ncom = 4 & ncom = 3 & ncom = 4 \\ \hline
    1052 & 16.18 & 219.14 & 14.44 & 160.55 \\ \hline
    1978 & 23.68 & 405.71 & 22.41 & 308.91 \\ \hline
    5235 & 43.6 & 1131.1 & 43.04 & 829.46 \\ \hline
    10420 & 83.44 & 2257.97 & 76.88 & 1702.04 \\ \hline
    19982 & 137.82 & 4374.85 & 118.79 & 3286.06 \\ \hline
    51170 & 247.5 & 11234.3 & 231.66 & 8520 \\ \hline
    102045 & 388.83 & 22602.43 & 370.6 & 17072.23 \\ \hline
    203163 & 608.21 & 45163.6 & 595.13 & 34011.8 \\ \hline
    \end{tabular}%
\end{table}

\subsubsection{Bandwidth Measurement}
The change in the \texttt{ncommonnodes} produce correct error values for the MPI MIDG2 designs but
for the IO channels design, slightly wrong error values are produced. This could be due some alignment
issues but the issue was not analysed in detail due to time limitation and as it just to simulate
higher data transfers and would not be used in a real system. The measurements for the bandwidth
were taken with MIDG2 MPI FPGA (as the base) and the IO channels designs only. As these
design share common execution sequence controlled by host, comparison of these designs
was identified to be more relevant. Also, the bandwidth computation in FPGA only design
is quite complex as there are not host interactions per iterations and clear waiting times
and active times are hard to predict from the profile information.
The bandwidth measurements for the MIDG2 MPI FPGA implementations are done using the \texttt{clock\_gettime()}
by measuring the total communication time for send and receive and computing the bandwidth similar to
bandwidth measurements for the topology evaluation. The times are store in the profile infor structure
which is processed at the end of every run. The code snippet in \ref{code:bwmpi_comp} shows the points in the different
routines where the measurement are added.
\begin{CppCode}[caption=Send and receive communication time computation changes for MIDG2 MPI FPGA, frame=tlrb, label=code:bwmpi_comp]
static void runKernel(...)
{
    ....

    if (mesh->nprocs > 1)
    {
    clock_gettime(CLOCK_MONOTONIC, &start);
    SendMpiData(mesh, clObjs);
    ....
    ....
    RecvMpiData(mesh, clObjs);
    clObjs->pKernel_obj->waitforcompletion("compute", &event1);
    ....
    }

    clObjs->pKernel_obj->enqueue_NDRange("compute",, , , , , , ,); // VolumeKernel

    clObjs->pKernel_obj->waitforcompletion("compute", &event1);

    profileInfo->sendExecTime += (cl_double)(diff(start, SendEnd)) * (cl_double)(1e-06);
    profileInfo->recvExecTime += (cl_double)(diff(start, RecvEnd))* (cl_double)(1e-06);
    ...
}

static void RecvMpiData(...)
{
 ...
    MPI_Status *instatus = (MPI_Status *)calloc(nprocs, sizeof(MPI_Status));
    MPI_Status *outstatus = (MPI_Status *)calloc(nprocs, sizeof(MPI_Status));

    MPI_Waitall(Nmess, mpi_out_requests, outstatus);
    clock_gettime(CLOCK_MONOTONIC, &SendEnd);

    MPI_Waitall(Nmess, mpi_in_requests, instatus);

    if (mesh->parNtotalout)
    {
        ret = clEnqueueWriteBuffer(, partQ_mem,,, sharedQMemSize, f_inQ,,,);
        if (ret != CL_SUCCESS)
            printf("recv Error %d\n", ret);

        clFinish(clObjs->cmd_queue_main);
        clock_gettime(CLOCK_MONOTONIC, &RecvEnd);
    }
...
}
\end{CppCode}

For the IO channels design, the measurement is simpler. The kernel exectution time
for \texttt{partial\_send} and \texttt{partial\_recv} kernels is comuputed by requesting
the \texttt{CL\_PROFILING\_COMMAND\_START} and \texttt{CL\_PROFILING\_COMMAND\_END} profile counters
as explained done in the prototypes. The communication times are then computed and stored
in the profileinfo buffers as shown in the code listing \ref{code:bwioch_comp}.
\begin{CppCode}[caption=Send and receive communication time computation changes for IO channels designs, frame=tlrb, label=code:bwioch_comp]
static void runKernel(...)
{
...
...
    clObjs->pKernel_obj->enqueue_NDRange("compute_inner",  ,,, , , , );

    clObjs->pKernel_obj->waitforcompletion("compute_inner", &event1);
...
    clObjs->pKernel_obj->enqueue_NDRange("compute_halo", , , , , , , , );
    clObjs->pKernel_obj->waitforcompletion("compute_halo", &event1);

    cl_ulong start = 0, end = 0;
    start = getProfileInfo("maxwell_partial_send", CL_PROFILING_COMMAND_START);
    end = getProfileInfo("maxwell_partial_send", CL_PROFILING_COMMAND_END);

    profileInfo->sendExecTime += (cl_double)(end - start) * (cl_double)(1e-06);

    start = getProfileInfo("maxwell_partial_rcv", CL_PROFILING_COMMAND_START);
    end = getProfileInfo("maxwell_partial_rcv", CL_PROFILING_COMMAND_END);

    profileInfo->recvExecTime += (cl_double)(end - start) * (cl_double)(1e-06);
...
}
\end{CppCode}


\subsubsection{Analysis of the results}

The plots in the figure \ref{plot:bw_avg} show the comparision of the average bandwidth
values recorded for the MPI and the IO designs. Figure (a) shows the bandwidth for
2 nodes whereas (b) shows for the 4 node designs. The first thing which is clearly visible
is the huge difference in the bandwidth values for the MPI and IO channels design which
proof the higher efficiency of the IO channels in real application scenarios as well.
The MPI reports a maximum bandwidth of 0.76 GB/s for the  200k Mesh in 2 node design with 18.0.1 toolchain.
which is only 25\% of the available bandwidth, whereas the within node design is able to
utilize the 70\% of the available bandwidth reporting a maximum bandwidth of 3.50 GB/s.
The Fully connected design as expectation have higher bandwidth values due to utilization
of 3 channels though the peak bandwidth of 6.09 for 200k Mesh is only 40\% which could be
due to the global memory access limitations. As all the kernels compute parallelly now,
the load on the global memory is higher for handling the requests which would lead to stalls and
explain the bahaviour.
\begin{figure}[ht]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/mpiio_bw_1.pgf}} & \scalebox{0.5}{\input{data/mpiio_bw_2.pgf}}\\
    (a) 2 node designs & (b) 4 Node Designs
	\end{tabular}
    \caption{Average bandwidth values for two node and 4 Node designs}
	\label{plot:bw_avg}
\end{figure}
The 4 channel design performs slower than the single channel design. As identified in
the topology testing the, performance of 4 channel design is affected due to the
wider global memory access which cause higher stalls. In the actual application,
with other kernels working simulataneously, the effects of the global memory has
increased leading to bandwidths slower than the single channel implementation.


The plots in figure \ref{plot:bw_ratio} compare the bandwidth speed up of
send and receive seprately. The receives is both IO channels design have significantly
higher speed up values then the sends. The difference can be accounted to the sharing of
global memory buffer in the send and hence the memory band with other kernels. The \texttt{partial\_send}
kernels reads the \texttt{g\_Q} memory buffer to get the shared data values. With the addition
of channels between rk and volume/surface kernel, the concurrent access of the \texttt{g\_Q}
increases which would cause stalls and delays in the send kernel leading to slower sends.
\begin{figure}[ht]
	\centering\small
	\begin{tabular}{cc}
    \scalebox{0.5}{\input{data/bwspeed_1.pgf}} & \scalebox{0.5}{\input{data/bwspeed_2.pgf}}\\
    (a) 2 node designs & (b) 4 Node Designs
	\end{tabular}
    \caption{Average bandwidth values for two node and 4 Node designs}
	\label{plot:bw_ratio}
\end{figure}
Looking at the plots it can also be clearly seen that performance of design synthesized
with 18.0.1 toolchains is lesser as the MPI design performs better with the 18.0.1
toolchain reporting higher bandwidth values. Although this behavior can only be seen in
the within the node topology. The fully connected topology have similar performance in both
toolchains.


\subsubsection{Execution time comparision}

The plot in figure \ref{plot:bw_speedup} show the comparision of the
execution time speedup for the designs used for bandwidth evaulation.
A maximum speed up value of 2.92 is reported for the fully connected
designs with both within node and fully connected topology
having speed of 2 and above for higher mesh sizes. This shows the potential
of using the IO channels with higher data transfer sizes which was not clear
with the exectution time evaluation. The within node 4 channel design has
speed ups below 2, mostly due to the global memory congestion in the send
pasrt as noticed with the earlier analysis.
\begin{figure}[ht]
	\centering\small
    \scalebox{0.8}{\input{data/bwspeed_com.pgf}}
    \caption{Speed up comparision for simulated higher data size transfers}
	\label{plot:bw_speedup}
\end{figure}

The fully conneted design synthesized with the 18.0.1 version is the only
18.0.1 design which performs better. After further analyis it was identified that
the difference could be due to addition of write-ack LSU for the \texttt{g\_resQ}
buffer in the the RK kernel which is solved in the FPGA only designs by aliasing
the memory.


% \subsection{Global Bandwidth}


% lower occupancy in the computation kernels giving hints to be computational
% bound



